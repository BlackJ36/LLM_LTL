"""
Mini Training Test
å°è§„æ¨¡è®­ç»ƒæµ‹è¯•ï¼ŒéªŒè¯å®Œæ•´è®­ç»ƒæµç¨‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""
import warnings
warnings.filterwarnings('ignore', category=DeprecationWarning)

import numpy as np
import torch
import time


class DummyEnv:
    """ç”¨äºæµ‹è¯•çš„è™šæ‹Ÿç¯å¢ƒ"""
    def __init__(self, obs_dim=32, action_dim=8):
        self.obs_dim = obs_dim
        self.action_dim = action_dim

    @property
    def observation_space(self):
        class Space:
            def __init__(self, dim):
                self.shape = (dim,)
                self.low = -np.inf * np.ones(dim)
                self.high = np.inf * np.ones(dim)
        return Space(self.obs_dim)

    @property
    def action_space(self):
        class Space:
            def __init__(self, dim):
                self.shape = (dim,)
                self.low = -np.ones(dim)
                self.high = np.ones(dim)
        return Space(self.action_dim)


def test_sac_training_loop():
    """æµ‹è¯•å®Œæ•´çš„SACè®­ç»ƒå¾ªç¯"""
    print("=" * 60)
    print("  SAC Training Loop Test")
    print("=" * 60)

    from maple.torch.sac.sac import SACTrainer
    from maple.torch.networks.mlp import ConcatMlp
    from maple.torch.sac.policies import TanhGaussianPolicy
    from maple.data_management.simple_replay_buffer import SimpleReplayBuffer
    import maple.torch.pytorch_util as ptu

    # è®¾ç½®GPUæ¨¡å¼ - è¿™å¯¹äºptuå†…éƒ¨çš„tensoråˆ›å»ºå¾ˆé‡è¦
    use_cuda = torch.cuda.is_available()
    if use_cuda:
        ptu.set_gpu_mode(True, 0)
    device = torch.device("cuda" if use_cuda else "cpu")
    print(f"Using device: {device}")

    # ç¯å¢ƒå‚æ•°
    obs_dim = 32
    action_dim = 8

    # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
    dummy_env = DummyEnv(obs_dim, action_dim)

    # åˆ›å»ºç½‘ç»œ
    hidden_sizes = [256, 256]

    qf1 = ConcatMlp(
        input_size=obs_dim + action_dim,
        output_size=1,
        hidden_sizes=hidden_sizes,
    ).to(device)

    qf2 = ConcatMlp(
        input_size=obs_dim + action_dim,
        output_size=1,
        hidden_sizes=hidden_sizes,
    ).to(device)

    target_qf1 = ConcatMlp(
        input_size=obs_dim + action_dim,
        output_size=1,
        hidden_sizes=hidden_sizes,
    ).to(device)

    target_qf2 = ConcatMlp(
        input_size=obs_dim + action_dim,
        output_size=1,
        hidden_sizes=hidden_sizes,
    ).to(device)

    policy = TanhGaussianPolicy(
        obs_dim=obs_dim,
        action_dim=action_dim,
        hidden_sizes=hidden_sizes,
    ).to(device)

    print("âœ“ Networks created")

    # åˆ›å»ºTrainer
    trainer = SACTrainer(
        env=dummy_env,
        policy=policy,
        qf1=qf1,
        qf2=qf2,
        target_qf1=target_qf1,
        target_qf2=target_qf2,
        discount=0.99,
        soft_target_tau=0.005,
        policy_lr=3e-4,
        qf_lr=3e-4,
    )
    print("âœ“ SACTrainer created")

    # åˆ›å»ºReplay Buffer
    replay_buffer = SimpleReplayBuffer(
        max_replay_buffer_size=10000,
        observation_dim=obs_dim,
        action_dim=action_dim,
        env_info_sizes={},
    )
    print("âœ“ ReplayBuffer created")

    # æ¨¡æ‹Ÿæ•°æ®æ”¶é›†
    print("\nSimulating data collection...")
    num_samples = 2000
    for i in range(num_samples):
        obs = np.random.randn(obs_dim).astype(np.float32)
        action = np.random.randn(action_dim).astype(np.float32)
        next_obs = np.random.randn(obs_dim).astype(np.float32)
        reward = np.array([np.random.randn()], dtype=np.float32)
        terminal = np.array([0.0], dtype=np.float32)

        replay_buffer.add_sample(
            observation=obs,
            action=action,
            reward=reward[0],
            terminal=terminal[0],
            next_observation=next_obs,
            env_info={},
        )

    print(f"âœ“ Added {num_samples} samples to replay buffer")

    # æ‰§è¡Œè®­ç»ƒ
    print("\nRunning training updates...")
    batch_size = 256
    num_updates = 100

    start_time = time.time()
    losses = []

    for i in range(num_updates):
        batch = replay_buffer.random_batch(batch_size)

        # è½¬æ¢ä¸ºtorch tensorå¹¶ç§»åˆ°GPU (ç¡®ä¿float32ç±»å‹)
        batch_torch = {
            'observations': torch.from_numpy(batch['observations']).float().to(device),
            'actions': torch.from_numpy(batch['actions']).float().to(device),
            'rewards': torch.from_numpy(batch['rewards']).float().to(device),
            'terminals': torch.from_numpy(batch['terminals']).float().to(device),
            'next_observations': torch.from_numpy(batch['next_observations']).float().to(device),
        }

        trainer.train_from_torch(batch_torch)

        if (i + 1) % 20 == 0:
            stats = trainer.get_diagnostics()
            qf_loss = stats.get('QF1 Loss', 0)
            policy_loss = stats.get('Policy Loss', 0)
            print(f"  Update {i+1}/{num_updates}: QF Loss={qf_loss:.4f}, Policy Loss={policy_loss:.4f}")
            losses.append(qf_loss)

    elapsed = time.time() - start_time
    print(f"\nâœ“ Training completed: {num_updates} updates in {elapsed:.2f}s")
    print(f"âœ“ Updates per second: {num_updates / elapsed:.1f}")

    return True


def test_robosuite_env():
    """æµ‹è¯•Robosuiteç¯å¢ƒ"""
    print("\n" + "=" * 60)
    print("  Robosuite Environment Test")
    print("=" * 60)

    try:
        import robosuite as suite

        # mapleåˆ†æ”¯éœ€è¦skill_config
        skill_config = dict(
            skills=['atomic', 'reach', 'grasp', 'push'],
            aff_penalty_fac=15.0,
            base_config=dict(
                global_xyz_bounds=[[-0.30, -0.30, 0.80], [0.15, 0.30, 0.95]],
                lift_height=0.95,
                binary_gripper=True,
                aff_threshold=0.06,
                aff_type='dense',
                aff_tanh_scaling=10.0,
            ),
            atomic_config=dict(use_ori_params=True),
            reach_config=dict(
                use_gripper_params=False,
                local_xyz_scale=[0.0, 0.0, 0.06],
                use_ori_params=False,
                max_ac_calls=15,
            ),
            grasp_config=dict(
                global_xyz_bounds=[[-0.30, -0.30, 0.80], [0.15, 0.30, 0.85]],
                aff_threshold=0.03,
                local_xyz_scale=[0.0, 0.0, 0.0],
                use_ori_params=True,
                max_ac_calls=20,
                num_reach_steps=2,
                num_grasp_steps=3,
            ),
            push_config=dict(
                global_xyz_bounds=[[-0.30, -0.30, 0.80], [0.15, 0.30, 0.85]],
                delta_xyz_scale=[0.25, 0.25, 0.05],
                max_ac_calls=20,
                use_ori_params=True,
                aff_threshold=[0.12, 0.12, 0.04],
            ),
        )

        # åˆ›å»ºç®€å•çš„Liftç¯å¢ƒ
        env = suite.make(
            env_name="Lift",
            robots="Panda",
            has_renderer=False,
            has_offscreen_renderer=False,
            use_camera_obs=False,
            control_freq=20,
            skill_config=skill_config,
        )

        print(f"âœ“ Environment created: Lift with Panda robot")
        print(f"  - Observation space dim: {env.observation_spec()}")
        action_low, action_high = env.action_spec
        action_dim = len(action_low)
        print(f"  - Action space dim: {action_dim}")

        # è¿è¡Œå‡ æ­¥
        obs = env.reset()
        print(f"âœ“ Environment reset, obs keys: {list(obs.keys())}")

        total_reward = 0
        for step in range(10):
            action = np.random.uniform(-1, 1, action_dim)
            obs, reward, done, info = env.step(action)
            total_reward += reward

        print(f"âœ“ Ran 10 steps, total reward: {total_reward:.4f}")

        env.close()
        print("âœ“ Environment closed")

        return True

    except Exception as e:
        print(f"âŒ Robosuite test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_maple_with_robosuite():
    """æµ‹è¯•Mapleä¸Robosuiteçš„é›†æˆ"""
    print("\n" + "=" * 60)
    print("  Maple + Robosuite Integration Test")
    print("=" * 60)

    try:
        import robosuite as suite
        from maple.torch.sac.policies import TanhGaussianPolicy
        from maple.torch.networks.mlp import Mlp
        import maple.torch.pytorch_util as ptu

        use_cuda = torch.cuda.is_available()
        if use_cuda:
            ptu.set_gpu_mode(True, 0)
        device = torch.device("cuda" if use_cuda else "cpu")

        # mapleåˆ†æ”¯éœ€è¦skill_config
        skill_config = dict(
            skills=['atomic', 'reach', 'grasp', 'push'],
            aff_penalty_fac=15.0,
            base_config=dict(
                global_xyz_bounds=[[-0.30, -0.30, 0.80], [0.15, 0.30, 0.95]],
                lift_height=0.95,
                binary_gripper=True,
                aff_threshold=0.06,
                aff_type='dense',
                aff_tanh_scaling=10.0,
            ),
            atomic_config=dict(use_ori_params=True),
            reach_config=dict(
                use_gripper_params=False,
                local_xyz_scale=[0.0, 0.0, 0.06],
                use_ori_params=False,
                max_ac_calls=15,
            ),
            grasp_config=dict(
                global_xyz_bounds=[[-0.30, -0.30, 0.80], [0.15, 0.30, 0.85]],
                aff_threshold=0.03,
                local_xyz_scale=[0.0, 0.0, 0.0],
                use_ori_params=True,
                max_ac_calls=20,
                num_reach_steps=2,
                num_grasp_steps=3,
            ),
            push_config=dict(
                global_xyz_bounds=[[-0.30, -0.30, 0.80], [0.15, 0.30, 0.85]],
                delta_xyz_scale=[0.25, 0.25, 0.05],
                max_ac_calls=20,
                use_ori_params=True,
                aff_threshold=[0.12, 0.12, 0.04],
            ),
        )

        # åˆ›å»ºç¯å¢ƒ
        env = suite.make(
            env_name="Lift",
            robots="Panda",
            has_renderer=False,
            has_offscreen_renderer=False,
            use_camera_obs=False,
            control_freq=20,
            skill_config=skill_config,
        )

        # è·å–ç»´åº¦
        obs = env.reset()
        obs_dim = sum([v.shape[0] if len(v.shape) > 0 else 1 for v in obs.values()])
        action_low, action_high = env.action_spec
        action_dim = len(action_low)

        print(f"âœ“ Env dimensions: obs={obs_dim}, action={action_dim}")

        # åˆ›å»ºç­–ç•¥ç½‘ç»œ
        policy = TanhGaussianPolicy(
            obs_dim=obs_dim,
            action_dim=action_dim,
            hidden_sizes=[256, 256],
        ).to(device)

        print("âœ“ Policy network created")

        # ä½¿ç”¨ç­–ç•¥è¿›è¡ŒåŠ¨ä½œé‡‡æ ·
        obs_flat = np.concatenate([v.flatten() for v in obs.values()])
        obs_tensor = torch.from_numpy(obs_flat).float().unsqueeze(0).to(device)

        with torch.no_grad():
            dist = policy(obs_tensor)
            action, log_prob = dist.rsample_and_logprob()

        print(f"âœ“ Policy forward pass: action shape={action.shape}")

        # è¿è¡Œä¸€ä¸ªepisode
        total_reward = 0
        obs = env.reset()
        for step in range(50):
            obs_flat = np.concatenate([v.flatten() for v in obs.values()])
            obs_tensor = torch.from_numpy(obs_flat).float().unsqueeze(0).to(device)

            with torch.no_grad():
                dist = policy(obs_tensor)
                action = dist.sample()

            action_np = action.cpu().numpy().flatten()
            obs, reward, done, info = env.step(action_np)
            total_reward += reward

            if done:
                break

        print(f"âœ“ Ran episode: {step+1} steps, total reward: {total_reward:.4f}")

        env.close()
        return True

    except Exception as e:
        print(f"âŒ Integration test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("\n" + "#" * 60)
    print("#" + "  Mini Training Test Suite".center(58) + "#")
    print("#" * 60 + "\n")

    results = {}

    # æµ‹è¯•SACè®­ç»ƒå¾ªç¯
    try:
        results["sac_training"] = test_sac_training_loop()
    except Exception as e:
        print(f"âŒ SAC training test error: {e}")
        import traceback
        traceback.print_exc()
        results["sac_training"] = False

    # æµ‹è¯•Robosuiteç¯å¢ƒ
    try:
        results["robosuite_env"] = test_robosuite_env()
    except Exception as e:
        print(f"âŒ Robosuite env test error: {e}")
        import traceback
        traceback.print_exc()
        results["robosuite_env"] = False

    # æµ‹è¯•é›†æˆ
    try:
        results["integration"] = test_maple_with_robosuite()
    except Exception as e:
        print(f"âŒ Integration test error: {e}")
        import traceback
        traceback.print_exc()
        results["integration"] = False

    # æ€»ç»“
    print("\n" + "#" * 60)
    print("#" + "  Test Summary".center(58) + "#")
    print("#" * 60)

    all_passed = True
    for test_name, passed in results.items():
        status = "âœ“ PASSED" if passed else "âŒ FAILED"
        print(f"  {test_name}: {status}")
        if not passed:
            all_passed = False

    print()
    if all_passed:
        print("  ğŸ‰ All mini training tests passed!")
        print("  The training pipeline is ready to use.")
    else:
        print("  âš ï¸ Some tests failed. Check the errors above.")

    print("#" * 60 + "\n")
